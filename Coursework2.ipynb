{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------\n",
    "# INM432 Big Data - Coursework 2\n",
    "#### Implementing a Recommender System on the MovieLens 20M dataset\n",
    "#### Adrian Ellis and Toby Staines\n",
    "-----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) Load the MovieLens dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[movieID: int, title: string, genres: string]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pixiedust\n",
    "import os\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import avg, sqrt\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# load movie ratings data\n",
    "ratings_data = 'hdfs://saltdean/data/movielens/ml-20m/ratings.csv'\n",
    "\n",
    "# define schema (note that inferSchema=\"true\" requires spark to read entire dataset)\n",
    "ratings_fields = [('userID', IntegerType()), ('movieID', IntegerType()), \n",
    "                  ('rating', FloatType()), ('timestamp', IntegerType())]\n",
    "# note nullable set to False. Leave timestamp as integer type as we are not using it\n",
    "fields = [StructField(field_name, field_type, False) for\n",
    "          field_name, field_type in ratings_fields]\n",
    "schema = StructType(fields)\n",
    "\n",
    "ratings = spark.read.load(ratings_data, format=\"csv\", sep=\",\", schema=schema, header=\"true\")\n",
    "#note above function is documented under pyspark.sql.DataFrameReader()\n",
    "ratings.drop('timestamp') # drop column as not required\n",
    "\n",
    "# load movie titles data\n",
    "movies_data = 'hdfs://saltdean/data/movielens/ml-20m/movies.csv'\n",
    "\n",
    "# define schema\n",
    "movies_fields = [('movieID', IntegerType()), ('title', StringType()), ('genres', StringType())]\n",
    "fields = [StructField(field_name, field_type, False) for\n",
    "          field_name, field_type in movies_fields]\n",
    "schema = StructType(fields)\n",
    "\n",
    "movies = spark.read.load(movies_data, format=\"csv\", sep=\",\", schema=schema, header=\"true\")\n",
    "# note that delimiters inside the default [\"] quote character are ignored which is what we want\n",
    "movies.drop('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000263\n",
      "[('userID', 'int'), ('movieID', 'int'), ('rating', 'float'), ('timestamp', 'int')]\n",
      "[Row(userID=1, movieID=2, rating=3.5, timestamp=1112486027), Row(userID=1, movieID=29, rating=3.5, timestamp=1112484676), Row(userID=1, movieID=32, rating=3.5, timestamp=1112484819), Row(userID=1, movieID=47, rating=3.5, timestamp=1112484727), Row(userID=1, movieID=50, rating=3.5, timestamp=1112484580)]\n",
      "27278\n",
      "[('movieID', 'int'), ('title', 'string'), ('genres', 'string')]\n",
      "[Row(movieID=1, title='Toy Story (1995)', genres='Adventure|Animation|Children|Comedy|Fantasy'), Row(movieID=2, title='Jumanji (1995)', genres='Adventure|Children|Fantasy'), Row(movieID=3, title='Grumpier Old Men (1995)', genres='Comedy|Romance'), Row(movieID=4, title='Waiting to Exhale (1995)', genres='Comedy|Drama|Romance'), Row(movieID=5, title='Father of the Bride Part II (1995)', genres='Comedy')]\n",
      "CPU times: user 24 ms, sys: 36 ms, total: 60 ms\n",
      "Wall time: 27.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# check dataframes loaded properly\n",
    "dfs = [ratings, movies]\n",
    "for df in dfs:\n",
    "    print(df.count())\n",
    "    print(df.dtypes)\n",
    "    print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other things we could do:\n",
    "- Use Dataframe SQL statements to print dimensions of data\n",
    "- Use pixiedust to show histogram of number of ratings by user\n",
    "- Calculate dataset sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings downsampled to 100187 for testing purposes\n",
      "CPU times: user 24 ms, sys: 8 ms, total: 32 ms\n",
      "Wall time: 15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "DEBUGGING = True\n",
    "# Downsample ratings dataframe for development\n",
    "if DEBUGGING:\n",
    "    ratings = ratings.sample(withReplacement=False, fraction=0.005, seed=0) #Set seed for reproducibiity\n",
    "    print(\"Ratings downsampled to {} for testing purposes\".format(ratings.count()))\n",
    "\n",
    "# split ratings dataframe into training and test sets\n",
    "# Note - ideally we would split the dataset according to a proportion of the userIDs\n",
    "ratings_train, ratings_test = ratings.randomSplit([0.9, 0.1], seed=0)\n",
    "# cache and save dataframes to reduce execution time\n",
    "ratings_train.cache()\n",
    "ratings_test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DELETE CELL IF NOT REQUIRED / CAN'T GET TO WORK\n",
    "# Can't get saving to parquet to work atm - try os(path.join()?)\n",
    "# ! rmdir 'ratings_train.pqt' # delete if they already exist to avoid an error below\n",
    "# ! rmdir 'ratings_test.pqt'\n",
    "# ratings_train.write.parquet('ratings_train.pqt')\n",
    "# ratings_test.write.parquet('ratings_test.pqt')\n",
    "# # note mode='overwrite' raises error as trying to overwrite same directory as deleting - needs a temporary directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Test ALS model (without pipeline)\n",
    "We will use Alternative Least Squares (ALS) to create a recommendation model using latent factors\n",
    "\n",
    "First implement without a pipeline so we explore the results qualitatively by calling methods of the ALS Model (note when using a pipeline we don't seem to have access to these methods as a Pipeline Model object is returned)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28 ms, sys: 0 ns, total: 28 ms\n",
      "Wall time: 4.03 s\n"
     ]
    }
   ],
   "source": [
    "als = ALS(userCol=\"userID\", itemCol=\"movieID\", ratingCol=\"rating\",\n",
    "          rank=10, maxIter=10, regParam=0.1, coldStartStrategy='drop', seed=0)\n",
    "          # use coldStartStrategy='drop' above to avoid 'nan' values for IDs not seen in training data\n",
    "    \n",
    "# fit the model\n",
    "%time model=als.fit(ratings_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1) Evaluate the prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 23.6 ms\n",
      "+------+---------------------------+------+-----------+\n",
      "|userID|title                      |rating|prediction |\n",
      "+------+---------------------------+------+-----------+\n",
      "|109469|Hudsucker Proxy, The (1994)|4.0   |2.974759   |\n",
      "|67352 |Hudsucker Proxy, The (1994)|3.5   |2.2586203  |\n",
      "|106543|High School High (1996)    |3.0   |-0.05509311|\n",
      "|106572|Dirty Dancing (1987)       |3.0   |-0.9867803 |\n",
      "|31550 |Dirty Dancing (1987)       |4.5   |-0.06630593|\n",
      "+------+---------------------------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%time predictions = model.transform(ratings_test)\n",
    "# Join ratings with movie title\n",
    "predictions = predictions.join(movies, predictions.movieID == movies.movieID, 'inner') \\\n",
    "                         .select('userID', 'title', 'rating', 'prediction')\n",
    "predictions.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------------------+------+----------+\n",
      "|userID|title                      |rating|prediction|\n",
      "+------+---------------------------+------+----------+\n",
      "|109469|Hudsucker Proxy, The (1994)|4.0   |2.974759  |\n",
      "|109469|Gilda (1946)               |4.0   |2.6662097 |\n",
      "+------+---------------------------+------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(recommendations=[Row(movieID=8183, rating=4.394659996032715), Row(movieID=5135, rating=4.393545150756836), Row(movieID=56949, rating=4.320899963378906), Row(movieID=71108, rating=4.23151969909668), Row(movieID=73881, rating=4.0969929695129395), Row(movieID=5644, rating=4.080875396728516), Row(movieID=57183, rating=4.0242509841918945), Row(movieID=8957, rating=3.9853291511535645), Row(movieID=7215, rating=3.981336832046509), Row(movieID=2365, rating=3.950181722640991)])]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Qualitative results\n",
    "# Show predictions vs. ratings for first userID\n",
    "firstID = predictions.first().userID\n",
    "firstDF = predictions.filter(predictions.userID == firstID)\n",
    "firstDF.show(truncate=False)\n",
    "\n",
    "# Show top 10 recommendations for first userID\n",
    "firstRecs = model.recommendForUserSubset(firstDF, 10).select('recommendations')\n",
    "firstRecs.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Fix above output so we can display a list of the movie titles and predicted rating\n",
    "\n",
    "Other things we could do:\n",
    "- Compare predictions vs. ratings for the userID with the highest number of ratings\n",
    "- Calculate the MSRE by user and:\n",
    "- Show predictions vs. ratings for the best / worst userID error\n",
    "- Use pixiedust to plot a histogram of MSRE by userID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|SQRT(avg(SquaredError))|\n",
      "+-----------------------+\n",
      "|      2.565372059862994|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quantitative results\n",
    "# Calculate model accuracy\n",
    "predictions = predictions.withColumn('SquaredError', (predictions.rating - predictions.prediction)**2)\n",
    "predictions.select(sqrt(avg(predictions[\"SquaredError\"]))).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Find best model using ALS and ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline\n",
    "pipeline = Pipeline(stages=[als])\n",
    "#print (\"Pipeline:\",pipeline.explainParams())\n",
    "#print (\"ALS:\",als.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: test with different training set sizes either by:\n",
    "- running code below multiple times for each training set\n",
    "- if it works, adding a pipeline set for sampling the training set and addding this as a param to the paramgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set performance evaluator\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "#rmse = evaluator.evaluate(predictions)\n",
    "#print(\"Root-mean-square error = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create parameter grid over ALS rank and regularisation parameter\n",
    "# TODO: add sample size if we are doing it this way\n",
    "# regParam must be >=0 (default=0.1)\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(als.rank,[3, 5])\\\n",
    "    .addGrid(als.regParam,[0.2, 0.5])\\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Enter more values for param grid e.g.\n",
    "als.rank,[1, 2, 3, 5, 10, 20]\n",
    "als.regParam,[0, 0.1, 0.3, 0.5, 1, 5, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set train/validation split\n",
    "validator = TrainValidationSplit(trainRatio=0.9, seed=0)\\\n",
    "            .setEstimator(pipeline).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 376 ms, sys: 124 ms, total: 500 ms\n",
      "Wall time: 32.2 s\n"
     ]
    }
   ],
   "source": [
    "# Execute pipeline\n",
    "%time tunedModel = validator.fit(ratings_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function provided by Tillman\n",
    "def bestValidationParamters(validatedModel,parameterGrid):\n",
    "    \"\"\" Find the paramter map that produced the highest result in a validation (TrainValidationSplit or CrossValidation) \n",
    "        \n",
    "        Positional arguments:\n",
    "        validatedModel: the model returned by cv.fit() or tvs.fit()\n",
    "        parameterGrid: the paramterGrid used in the fitting\n",
    "    \"\"\"\n",
    "    # link the measured metric results to the paramter maps in the grid\n",
    "    metricParamPairs = zip(validatedModel.validationMetrics,parameterGrid)\n",
    "    # for our metrics, higher is better and 0 is the minimum\n",
    "    bestMetric = 0 # initialize with the minimal value\n",
    "    # now iterate through all tested parameter maps\n",
    "    for metric,params in metricParamPairs:\n",
    "        if metric > bestMetric: # if metric is better than current best\n",
    "            bestParams = params # then keept the corresponding parameter map \n",
    "    return bestParams # and return the final best paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='ALS_4ce88fef3c32d548d763', name='rank', doc='rank of the factorization'): 5,\n",
       " Param(parent='ALS_4ce88fef3c32d548d763', name='regParam', doc='regularization parameter (>= 0).'): 0.5}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find parameters for best model\n",
    "bestValidationParamters(tunedModel,paramGrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for tuned model = 0.6410849858403498\n",
      "Test accuracy for tuned model = 1.6411222575909927\n"
     ]
    }
   ],
   "source": [
    "# Calculate peformance for best model\n",
    "print(\"Training accuracy for tuned model =\",evaluator.evaluate(tunedModel.transform(ratings_train)))\n",
    "print(\"Test accuracy for tuned model =\",evaluator.evaluate(tunedModel.transform(ratings_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other things we can do:\n",
    "- Tabulate / plot performance by sample size and hyperparameter settings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
